{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c56dbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 480)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(r\"C:.\\Nutrition5K\\Nutrition5K\\train\\color\\dish_0000\\rgb.png\")\n",
    "print(img.size)  # 输出 (W, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a032ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBNReLU(nn.Module):\n",
    "    \"\"\"Basic conv block: Conv -> BN -> ReLU\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, k=3, s=1, p=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, k, stride=s, padding=p, bias=False)\n",
    "        self.bn   = nn.BatchNorm2d(out_ch, eps=1e-5, momentum=0.1)\n",
    "        self.act  = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "class TinyCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Ultra-small CNN for RGB-D regression.\n",
    "    - Early fusion: set in_ch=4 to accept RGB(3)+Depth(1)\n",
    "    - By default uses standard GAP (no mask).\n",
    "    - You can enable masked-GAP later via use_mask=True and passing a mask tensor to forward().\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=4, widths=(24, 48, 96, 128), dropout=0.3, out_dim=1, use_mask: bool=False):\n",
    "        super().__init__()\n",
    "        self.use_mask = use_mask\n",
    "        w1, w2, w3, w4 = widths\n",
    "\n",
    "        # Stem -> H/2\n",
    "        self.stem = nn.Sequential(\n",
    "            ConvBNReLU(in_ch, w1, k=3, s=2, p=1),\n",
    "            ConvBNReLU(w1, w1, k=3, s=1, p=1),\n",
    "        )\n",
    "        # Stage1 -> H/4\n",
    "        self.stage1 = nn.Sequential(\n",
    "            ConvBNReLU(w1, w2, k=3, s=2, p=1),\n",
    "            ConvBNReLU(w2, w2, k=3, s=1, p=1),\n",
    "        )\n",
    "        # Stage2 -> H/8\n",
    "        self.stage2 = nn.Sequential(\n",
    "            ConvBNReLU(w2, w3, k=3, s=2, p=1),\n",
    "            ConvBNReLU(w3, w3, k=3, s=1, p=1),\n",
    "        )\n",
    "        # Stage3 -> H/16\n",
    "        self.stage3 = nn.Sequential(\n",
    "            ConvBNReLU(w3, w4, k=3, s=2, p=1),\n",
    "            ConvBNReLU(w4, w4, k=3, s=1, p=1),\n",
    "        )\n",
    "\n",
    "        # Head: GAP/Masked-GAP -> Dropout -> MLP(128) -> Linear(out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1     = nn.Linear(w4, 128)\n",
    "        self.fc2     = nn.Linear(128, out_dim)  # regression head (no activation)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.stage1(x)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)   # [B, C, H/16, W/16]\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def masked_gap(feat: torch.Tensor, mask: torch.Tensor | None):\n",
    "        \"\"\"\n",
    "        Optional masked global average pooling.\n",
    "        If mask is None, this falls back to standard GAP outside this method.\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            # Should not be called when mask=None; kept for completeness.\n",
    "            return F.adaptive_avg_pool2d(feat, 1).flatten(1)\n",
    "        m = F.interpolate(mask, size=feat.shape[-2:], mode=\"nearest\").clamp(0, 1)  # [B,1,Hf,Wf]\n",
    "        num = (feat * m).sum(dim=(2,3))           # [B,C]\n",
    "        den = (m.sum(dim=(2,3)) + 1e-6)           # [B,1]\n",
    "        return num / den\n",
    "\n",
    "    def forward(self, x, mask: torch.Tensor | None = None):\n",
    "        \"\"\"\n",
    "        x:    [B, in_ch, H, W]  (e.g., in_ch=4 for RGB-D)\n",
    "        mask: [B, 1, H, W] or None. Ignored unless self.use_mask is True.\n",
    "        \"\"\"\n",
    "        f = self.forward_features(x)\n",
    "        if self.use_mask:\n",
    "            # Use masked-GAP ONLY when explicitly enabled and mask is provided.\n",
    "            pooled = self.masked_gap(f, mask)\n",
    "        else:\n",
    "            # Standard GAP (default baseline, no mask involved).\n",
    "            pooled = F.adaptive_avg_pool2d(f, 1).flatten(1)\n",
    "\n",
    "        z = self.dropout(pooled)\n",
    "        z = F.relu(self.fc1(z), inplace=True)\n",
    "        out = self.fc2(z)  # [B, out_dim]\n",
    "        return out\n",
    "\n",
    "# ------- tiny self-test -------\n",
    "if __name__ == \"__main__\":\n",
    "    B, H, W = 8, 640, 480\n",
    "    # Default: no mask path (use_mask=False)\n",
    "    model = TinyCNN(in_ch=4, widths=(24,48,96,128), dropout=0.3, out_dim=1, use_mask=False)\n",
    "    x = torch.randn(B, 4, H, W)\n",
    "    y = model(x)\n",
    "    print(\"Output (no mask) shape:\", y.shape)\n",
    "\n",
    "    # Later: enable masked-GAP easily\n",
    "    model_mask = TinyCNN(in_ch=4, widths=(24,48,96,128), dropout=0.3, out_dim=1, use_mask=True)\n",
    "    m = torch.randint(0, 2, (B, 1, H, W)).float()\n",
    "    y2 = model_mask(x, mask=m)\n",
    "    print(\"Output (with mask) shape:\", y2.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-advanced",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
